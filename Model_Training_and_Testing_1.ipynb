{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model Development\n",
        "\n",
        "*   Model Selection: For the study, ANN model would be used.\n",
        "*   Loss function Definition: sparse categorical crossentropy\n",
        "*   Model Training and Tuning\n",
        "\n",
        "**Limitation in this project**: Here, due to lack of resources, model tuning will not be performed."
      ],
      "metadata": {
        "id": "KFyQIopo8HHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s8N6zexEEW5",
        "outputId": "f08130f6-aeb7-4cb5-9655-402afb242558"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Crash severity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooMRXA28EKV9",
        "outputId": "63e90312-c6e0-4a51-a74d-a63baf13fb4a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1PPTdMShCmN_ebwQDrXakKkI4ggw-4Ayf/Crash severity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "y_train = pd.read_csv('y_train.csv')\n",
        "chunksize = 10000\n",
        "chunks = []\n",
        "for chunk in pd.read_csv('X_train.csv', chunksize=chunksize, dtype=np.float16):\n",
        "  chunks.append(chunk)\n",
        "X_train = pd.concat(chunks, ignore_index=True)"
      ],
      "metadata": {
        "id": "eT9kyS908QUH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Free memory by deleting variables from system memory\n",
        "for var in list(globals().keys()):\n",
        "  if var not in ['X_train','y_train'] and not var.startswith('__'):\n",
        "    del globals()[var]\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "print(globals().keys())"
      ],
      "metadata": {
        "id": "b5UGcnjeFg-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a76d112-2b91-45bd-b8d7-465758dad3d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '__', '___', 'y_train', 'X_train', 'var', 'gc'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def create_dataset(X, y, batch_size, is_training=True):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "  if is_training:\n",
        "    dataset = dataset.shuffle(buffer_size=len(X))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  return dataset\n"
      ],
      "metadata": {
        "id": "HLSp21VLxRpN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_size = int(0.7 * len(X_train))\n",
        "train_dataset = create_dataset(X_train[:train_size], y_train[:train_size], batch_size).apply(tf.data.experimental.prefetch_to_device('/gpu:0'))\n",
        "val_dataset = create_dataset(X_train[train_size:], y_train[train_size:], batch_size, is_training=False).apply(tf.data.experimental.prefetch_to_device('/gpu:0'))"
      ],
      "metadata": {
        "id": "aai_D8X_vtLd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Free memory by deleting variables from system memory\n",
        "for var in list(globals().keys()):\n",
        "  if var not in ['train_dataset','val_dataset'] and not var.startswith('__'):\n",
        "    del globals()[var]\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "print(globals().keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVdgtkwyx0nk",
        "outputId": "13e9e61d-bb53-4a1b-b639-ff1c59138b53"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '__', '___', 'train_dataset', 'val_dataset', 'var', 'gc'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZJbt5uVf8DsF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "for features, labels in train_dataset.take(1):\n",
        "  input_shape = features.shape[1:]\n",
        "\n",
        "inputs = tf.keras.Input(shape=input_shape)\n",
        "x = tf.keras.layers.Dense(64,activation='relu')(inputs)\n",
        "x = tf.keras.layers.Dense(64,activation='relu')(x)\n",
        "outputs = tf.keras.layers.Dense(4,activation='softmax')(x)\n",
        "model = tf.keras.Model(inputs,outputs)\n",
        "model.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change initial_epoch if system crashes"
      ],
      "metadata": {
        "id": "C8JtPIMWIM1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('best_model.keras')\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    initial_epoch = 8,\n",
        "    epochs = 10,\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath='best_model.keras',\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            mode='min',\n",
        "            verbose=1\n",
        "        ),\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience = 3\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4U2L8I7xx8s",
        "outputId": "6b2ad452-b108-4f8b-b7c9-a66537f432d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10\n",
            " 135247/Unknown \u001b[1m259s\u001b[0m 2ms/step - accuracy: 0.8014 - loss: 0.4773"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9: val_loss improved from inf to 0.47717, saving model to best_model.keras\n",
            "\u001b[1m135247/135247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 2ms/step - accuracy: 0.8014 - loss: 0.4773 - val_accuracy: 0.8019 - val_loss: 0.4772\n",
            "Epoch 10/10\n",
            "\u001b[1m135238/135247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8018 - loss: 0.4767\n",
            "Epoch 10: val_loss did not improve from 0.47717\n",
            "\u001b[1m135247/135247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 2ms/step - accuracy: 0.8018 - loss: 0.4767 - val_accuracy: 0.8002 - val_loss: 0.4777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Evaluation and Interpretation (1)\n",
        "\n",
        "*   Performance of the Best Model: The best model for ANN is loaded for evaluation.\n",
        "*   Error Analysis: Inclusion of other road characteristics data such as shoulder width, injury severity, different road characteristics, skid resistance would have enhanced the quality of this study. Additionally, a computer with better computational resources would also have helped in the study to delve deeper through tuning.\n",
        "*   Outcome Interpretation: ANNs being a \"black-box\" model incorporated with Shapley Additive exPlanations (SHAP) helps explaining the feature important for prediction of severity."
      ],
      "metadata": {
        "id": "T0pRjvZe8bx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(X, y, batch_size, is_training=True):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "  if is_training:\n",
        "    dataset = dataset.shuffle(buffer_size=len(X))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "reUev5MMjA27"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "X_test = pd.read_csv('X_test.csv',dtype=np.float16)\n",
        "y_test = pd.read_csv('y_test.csv')"
      ],
      "metadata": {
        "id": "2Vrf4tPDjKC1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = create_dataset(X_test, y_test, batch_size=32, is_training=False).apply(tf.data.experimental.prefetch_to_device('/gpu:0'))"
      ],
      "metadata": {
        "id": "JnlJ6g-qjl-y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('best_model.keras')\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJR6O2Y3j18G",
        "outputId": "60dec498-3014-4b7b-8ebe-0d0529207273"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m48303/48303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2ms/step - accuracy: 0.8018 - loss: 0.4777\n",
            "Test Loss: 0.4774448275566101\n",
            "Test Accuracy: 0.8020319938659668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        }
      ]
    }
  ]
}